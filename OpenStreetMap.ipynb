{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle OpenStreetMap Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to choose any area of the world in https://www.openstreetmap.org and use data munging techniques to clean the OpenStreetMap data. <br>\n",
    "<br>\n",
    "__Map Area__ <br>\n",
    "- Berkeley, CA, USA: https://mapzen.com/data/metro-extracts/your-extracts/23df9fff1a7c <br>\n",
    "I'm currently studying in UCB, so I'm more interested to see what database querying reveals about this area. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generate small samples to make auditing easier\n",
    "- Auditing the tag types, k tag types, and exploring users\n",
    "- Observing problems when auditing street names and postal codes\n",
    "- Cleaning dataset\n",
    "- Converting dataset from XML to CSV format and importing the cleaned .csv files into a SQL database\n",
    "- Overview statistics of the dataset\n",
    "- Additional ideas by exploring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsetting the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Due to the large size of dataset, I started out by looking at a smaller sample first when auditing it to make it easier to iterate on investigation. First, I used k = 100 to make sure that my code works, and then reduced value of k to check for the most common problems to clean. <br>\n",
    "The complete code for subsetting the data can be found here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auditing Tag Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get the feeling on how much of which data I can expect to have in this map area, I used the iterative parsing to process the map file and found out all types of tags there and the count for each tag. <br>\n",
    "The complete code for auditing the tag types can be found here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_tags(filename):\n",
    "    file = open(filename, \"r\")\n",
    "    tags = {}\n",
    "    for event, elem in ET.iterparse(file):\n",
    "        if elem.tag in tags.keys():\n",
    "            tags[elem.tag] += 1\n",
    "        else:\n",
    "            tags[elem.tag] = 1\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "{'bounds': 1,\n",
    " 'member': 7325,\n",
    " 'nd': 575011,\n",
    " 'node': 492980,\n",
    " 'osm': 1,\n",
    " 'relation': 588,\n",
    " 'tag': 185978,\n",
    " 'way': 69973}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reading through the documentation for OSM XML file format, I found that our data primitives are nodes, ways, and relations. <br>\n",
    "- a block of nodes containing especially the location in the WGS84 reference system\n",
    "    - the tags of each node\n",
    "- a block of ways\n",
    "    - the references to its nodes for each way\n",
    "    - the tags of each way\n",
    "- a block of relations\n",
    "    - the references to its members for each relation\n",
    "    - the tags of each relation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k Tag Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before I process the data and add it into my database, I also need to check the \"k\" value for each \"$<tag>$\" and see if there are any potential problems. <br>\n",
    "I used 3 regular expressions to check for certain patterns in the tags, so that I'll have a count of each of four tag categories in a dictionary:\n",
    "- \"lower\", for tags that contain only lowercase letters and are valid\n",
    "- \"lower_colon\", for otherwise valid tags with a colon in their names\n",
    "- \"problemchars\", for tags with problematic characters, and\n",
    "- \"other\", for other tags that do not fall into the other three categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, found a list of tags that matches above regular expressions. The complete code can be found here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def key_type(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "        val = element.get(\"k\")\n",
    "        if bool(lower.search(val)):\n",
    "            keys[\"lower\"] += 1\n",
    "        elif bool(lower_colon.search(val)):\n",
    "            keys[\"lower_colon\"] += 1\n",
    "        elif bool(problemchars.search(val)):\n",
    "            keys[\"problemchars\"] += 1\n",
    "        else:\n",
    "            keys[\"other\"] += 1\n",
    "        pass\n",
    "        \n",
    "    return keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "{'lower': 133257, 'lower_colon': 50572, 'other': 2149, 'problemchars': 0} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm also interested on how many unique users have contributed to the map in Berkeley area. <br>\n",
    "The complete code can be found here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_user(element):\n",
    "    if element.get('uid'):\n",
    "        uid = element.attrib[\"uid\"]\n",
    "        return uid\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def process_map(filename):\n",
    "    users = set()\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        if get_user(element):\n",
    "            users.add(get_user(element))\n",
    "        pass\n",
    "\n",
    "    return users\n",
    "\n",
    "users = process_map('berkeley_california.osm')\n",
    "pprint.pprint(users)\n",
    "print(len(users)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(users) = 555"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auditing Street Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To clean this dataset, I want to audit the street names in tags and return those unexpected street types to the appropriate ones in the expected list I created. <br>\n",
    "Notice that, the original expected list was not perfect, so we need to update it when auditing street names. <br>\n",
    "The complete code can be found here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\", \"Alameda\", \"Broadway\", \"Circle\", \"Freeway\", \"Hall\",\n",
    "            \"Highway\", \"Loma\", \"Path\", \"Plaza\", \"Steps\", \"Terrace\", \"View\", \"Walk\", \"Way\"]\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return street_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "{'10675': set(['10675']),\n",
    " '155': set(['University Ave #155']),\n",
    " '411': set(['Horton Street Unit #411']),\n",
    " 'Ave': set(['Shattuck Ave', 'Telegraph Ave']),\n",
    " 'Blvd': set(['Canal Blvd']),\n",
    " 'St': set(['2nd St'])} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the main problem for street names is that some of them are over abbreviated. The format of numbers can be treated as something like apartment number. So, here I choose to ignore them, and change the names that are over abbreviated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auditing Postal Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To clean this dataset, I also want to audit the postal codes in tags and return the postal codes that are not presented in appropriate way. <br>\n",
    "The postal code in Berkeley should be started with 947. Otherwise, it should be considered as inappropriate. <br>\n",
    "The complete code can be found here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def audit_zipcodes(osmfile):\n",
    "    # iter through all zip codes, collect all the zip codes that does not start with 947\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    zip_codes = {}\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if tag.attrib['k'] == \"addr:postcode\" and not tag.attrib['v'].startswith('947'):\n",
    "                    if tag.attrib['v'] not in zip_codes:\n",
    "                        zip_codes[tag.attrib['v']] = 1\n",
    "                    else:\n",
    "                        zip_codes[tag.attrib['v']] += 1\n",
    "    return zip_codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "94109 1 <br>\n",
    "94607 1 <br>\n",
    "94605 1 <br>\n",
    "95476 2 <br>\n",
    "94804 16 <br>\n",
    "94618 74 <br>\n",
    "94608 101 <br>\n",
    "94609 36 <br>\n",
    "93710 1 <br>\n",
    "94530 73 <br>\n",
    "94611 1075 <br>\n",
    "95430 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that almost every incorrect postal codes are started with 94, that means they are actually very near to Berkeley. <br>\n",
    "After checking all of those codes, I found that they are basically postal codes for Oakland, Richmond, and San Francisco, which are neighbors of Berkeley."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems Observed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the problems I observed: <br>\n",
    "- The format of street names is not uniform, because some of them are over abbreviated. I need to clean those later.\n",
    "- The other problem is some inappropriate postal codes, which actually from the neighbors of Berkeley."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Street Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because some street names are over abbreviated, I need to fix them by mapping to the expected list. <br>\n",
    "The complete code can be found here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mapping = { \"St\": \"Street\",\n",
    "            \"Ave\": \"Avenue\",\n",
    "            \"Blvd\": \"Boulevard\"\n",
    "            }\n",
    "def update_name(name, mapping):\n",
    "    m = street_type_re.search(name)\n",
    "    if m.group() not in expected:\n",
    "        if m.group() in mapping.keys():\n",
    "            name = re.sub(m.group(), mapping[m.group()], name)\n",
    "    return name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Canal Blvd => Canal Boulevard <br>\n",
    "Telegraph Ave => Telegraph Avenue <br>\n",
    "Shattuck Ave => Shattuck Avenue <br>\n",
    "2nd St => 2nd Street"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data Into SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After auditing is complete the next step is to prepare the data to be inserted into a SQL database. To do so I need to parse the elements in the OSM XML file, transforming them from document format to tabular format, thus making it possible to write to .csv files. These csv files can then easily be imported to a SQL database as tables. <br>\n",
    "The complete code can be found here: <br>\n",
    "Defined schema for the .csv files and the eventual tables can be found here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "                             \n",
    "SCHEMA = schema.schema\n",
    "\n",
    "# Make sure the fields order in the csvs matches the column order in the sql table schema\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                  problem_chars=PROBLEMCHARS, default_tag_type='regular'):\n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []  # Handle secondary tags the same way for both node and way elements\n",
    "\n",
    "    if element.tag == 'node':\n",
    "        for attrib in element.attrib:\n",
    "            if attrib in NODE_FIELDS:\n",
    "                node_attribs[attrib] = element.attrib[attrib]\n",
    "        for child in element:\n",
    "            node_tags = {}\n",
    "            if PROBLEMCHARS.match(child.attrib['k']):\n",
    "                continue\n",
    "            elif LOWER_COLON.match(child.attrib['k']):\n",
    "                node_tags['id'] = element.attrib['id']\n",
    "                node_tags['key'] = child.attrib['k'].split(\":\", 1)[1]\n",
    "                node_tags['type'] = child.attrib['k'].split(\":\", 1)[0]             \n",
    "                # Use cleaning function:\n",
    "                if child.attrib['k'] == 'addr:street':\n",
    "                    node_tags['value'] = update_name(child.attrib['v'], mapping)\n",
    "                else:\n",
    "                    node_tags['value'] = child.attrib['v']\n",
    "            else:\n",
    "                node_tags['type'] = 'regular'\n",
    "                node_tags['key'] = child.attrib['k']\n",
    "                node_tags['id'] = element.attrib['id']\n",
    "                # Use cleaning function:\n",
    "                if child.attrib['k'] == 'addr:street':\n",
    "                    node_tags['value'] = update_name(child.attrib['v'], mapping)\n",
    "                else:\n",
    "                    node_tags['value'] = child.attrib['v']\n",
    "            tags.append(node_tags)\n",
    "\n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "\n",
    "    elif element.tag == 'way':\n",
    "        for attrib in element.attrib:\n",
    "            if attrib in WAY_FIELDS:\n",
    "                way_attribs[attrib] = element.attrib[attrib]\n",
    "\n",
    "        count = 0\n",
    "        for child in element:\n",
    "            way_tag = {}\n",
    "            way_node = {}\n",
    "\n",
    "            if child.tag == 'tag':\n",
    "                if PROBLEMCHARS.match(child.attrib['k']):\n",
    "                    continue\n",
    "                elif LOWER_COLON.match(child.attrib['k']):\n",
    "                    way_tag['id'] = element.attrib['id']\n",
    "                    way_tag['key'] = child.attrib['k'].split(\":\", 1)[1]\n",
    "                    way_tag['type'] = child.attrib['k'].split(\":\", 1)[0]\n",
    "                    # Use cleaning function:\n",
    "                    if child.attrib['k'] == 'addr:street':\n",
    "                        way_tag['value'] = update_name(child.attrib['v'], mapping)\n",
    "                    else:\n",
    "                        way_tag['value'] = child.attrib['v']\n",
    "                else:\n",
    "                    way_tag['type'] = 'regular'\n",
    "                    way_tag['key'] = child.attrib['k']\n",
    "                    way_tag['id'] = element.attrib['id']\n",
    "                    # Use cleaning function:\n",
    "                    if child.attrib['k'] == 'addr:street':\n",
    "                        way_tag['value'] = update_name(child.attrib['v'], mapping)\n",
    "                    else:\n",
    "                        way_tag['value'] = child.attrib['v']\n",
    "                tags.append(way_tag)\n",
    "            elif child.tag == 'nd':\n",
    "                way_node['id'] = element.attrib['id']\n",
    "                way_node['node_id'] = child.attrib['ref']\n",
    "                way_node['position'] = count\n",
    "                count += 1\n",
    "                way_nodes.append(way_node)\n",
    "\n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used SQL queries are used to get a statistical overview of the dataset, like: <br>\n",
    "- size of the file\n",
    "- number of nodes and ways\n",
    "- number of unique users\n",
    "- number of chosen type of nodes, like cafes\n",
    "- top 5 contributing users\n",
    "- Popular cuisines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nodes.csv...............................: 38M  \n",
    "nodes_tags.csv..........................: 1M   \n",
    "OpenStreetMap.db........................: 56M  \n",
    "sample.osm..............................: 1M   \n",
    "ways.csv................................: 3M   \n",
    "ways_nodes.csv..........................: 13M  \n",
    "ways_tags.csv...........................: 4M  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlite> SELECT COUNT(*) FROM nodes;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "492980"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlite> SELECT COUNT(*) FROM ways;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "69973"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of unique users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlite> SELECT COUNT(DISTINCT(sub.uid))          \n",
    "   ...> FROM (SELECT uid FROM nodes UNION ALL SELECT uid FROM ways) sub;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "541"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of cafes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlite> SELECT value, COUNT(*)\n",
    "   ...> FROM (SELECT * FROM nodes_tags as NT UNION ALL\n",
    "   ...>       SELECT * FROM ways_tags as WT) as sub\n",
    "   ...> WHERE value = 'cafe';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cafe,185"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Popular cuisines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlite> SELECT NT.value, COUNT(*) as num\n",
    "   ...> FROM nodes_tags as NT\n",
    "   ...>      JOIN (SELECT DISTINCT(id) FROM nodes_tags WHERE value='restaurant') sub\n",
    "   ...>      ON NT.id = sub.id\n",
    "   ...> WHERE NT.key = 'cuisine'\n",
    "   ...> GROUP BY NT.value\n",
    "   ...> ORDER BY num DESC\n",
    "   ...> LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mexican,26 <br>\n",
    "pizza,26 <br>\n",
    "thai,24 <br>\n",
    "japanese,23 <br>\n",
    "american,15 <br>\n",
    "italian,15 <br>\n",
    "indian,14 <br>\n",
    "chinese,13 <br>\n",
    "burger,11 <br>\n",
    "sushi,9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 5 contributing users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlite> SELECT sub.user, COUNT(*) as num\n",
    "   ...> FROM (SELECT user FROM nodes UNION ALL\n",
    "   ...>       SELECT user FROM ways) sub\n",
    "   ...> GROUP BY sub.user\n",
    "   ...> ORDER BY num DESC\n",
    "   ...> LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "andygol,290847 <br>\n",
    "dannykath,82683 <br>\n",
    "oba510,33779 <br>\n",
    "dchiles,20177 <br>\n",
    "RichRico,18537"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlite> SELECT SUM(num) as Total\n",
    "   ...> FROM\n",
    "   ...> (SELECT sub.user, COUNT(*) as num\n",
    "   ...>  FROM (SELECT user FROM nodes UNION ALL SELECT user FROM ways) sub\n",
    "   ...>        GROUP BY sub.user\n",
    "   ...>        ORDER BY num DESC) U;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total contribution by all users = 562953"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Idea about Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By exploring the dataset, I noticed some percentage statistics: <br>\n",
    "- top 1 user (andygol) contribution percentage: 51.66%\n",
    "- top 5 users contribution percentage: 79.23%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that the top contributing users plays more important roles on the performance of data. I think that could make the data has some bias or mistakes caused by top contributing users. So, if user ID can be displayed and encourage others to make changes on information created by those top contributing users, then this map dataset would be more precise, I hope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Udacity\n",
    "- mablatnik's Github: https://github.com/mablatnik/Wrangle-OpenStreetMap-Data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
